{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sequence): #TESTED\n",
    "    \"\"\"\n",
    "    Translates a nucleotide sequence into an amino acid sequence.\n",
    "\n",
    "    Arguments:\n",
    "    sequence -- a string representing the nucleotide sequence\n",
    "\n",
    "    Returns:\n",
    "    A string representing the corresponding amino acid sequence\n",
    "    \"\"\"\n",
    "    codon_table = {\n",
    "        'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M',\n",
    "        'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n",
    "        'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K',\n",
    "        'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',\n",
    "        'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n",
    "        'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n",
    "        'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q',\n",
    "        'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n",
    "        'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',\n",
    "        'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n",
    "        'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E',\n",
    "        'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n",
    "        'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n",
    "        'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L',\n",
    "        'TAC':'Y', 'TAT':'Y', 'TAA':'_', 'TAG':'_',\n",
    "        'TGC':'C', 'TGT':'C', 'TGA':'_', 'TGG':'W',\n",
    "    }\n",
    "    protein = ''\n",
    "    if len(sequence) % 3 != 0:\n",
    "        print(\"Warning: Sequence length is not a multiple of 3.\")\n",
    "    for i in range(0, len(sequence), 3):\n",
    "        codon = sequence[i:i+3]\n",
    "        if codon in codon_table:\n",
    "            protein += codon_table[codon]\n",
    "        else:\n",
    "            protein += 'X'  # unknown amino acid\n",
    "    return protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0205950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codon frequencies based on Human table from GenScript https://www.genscript.com/tools/codon-frequency-table\n",
    "\n",
    "#These have been double checked for accuracy \n",
    "most_frequent_codon = {\"F\": \"TTC\", \"L\": \"CTG\", \"Y\": \"TAC\", \"_\":\"TGA\", \"H\":\"CAC\", \"Q\":\"CAG\", \"I\":\"ATC\", \"M\":\"ATG\", \n",
    "                      \"N\":\"AAC\", \"K\":\"AAG\", \"V\":\"GTG\", \"D\":\"GAC\", \"E\":\"GAG\", \"S\":\"AGC\", \"C\":\"TGC\", \"W\":\"TGG\",\n",
    "                       \"P\":\"CCC\", \"R\":\"CGG\", \"T\": \"ACC\", \"A\":\"GCC\", \"G\":\"GGC\"}\n",
    "\n",
    "#None means there is only one codon\n",
    "second_most_frequent_codon = {\"F\": \"TTT\", \"L\": \"CTC\", \"Y\": \"TAT\", \"_\":\"TAA\", \"H\":\"CAT\", \"Q\":\"CAA\", \"I\":\"ATT\", \"M\":None, \n",
    "                      \"N\":\"AAT\", \"K\":\"AAA\", \"V\":\"GTC\", \"D\":\"GAT\", \"E\":\"GAA\", \"S\":\"TCC\", \"C\":\"TGT\", \"W\":None,\n",
    "                       \"P\":\"CCT\", \"R\":\"AGA\", \"T\": \"ACA\", \"A\":\"GCT\", \"G\":\"GGG\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb139867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta_file(filename): #TESTED\n",
    "    \"\"\"\n",
    "    Reads a FASTA file and returns the sequence as a string.\n",
    "\n",
    "    Arguments:\n",
    "    filename -- the name of the FASTA file to read\n",
    "\n",
    "    Returns:\n",
    "    A string representing the sequence in the FASTA file.\n",
    "    \"\"\"\n",
    "    sequence = \"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.startswith(\">\"):\n",
    "                sequence += line.strip()\n",
    "    return sequence.upper() #return the sequence in upper case to ensure compatibility with the downstream code (e.g. genetic table search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fd871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_complement(sequence): #TESTED\n",
    "    \"\"\"\n",
    "    Returns the reverse complement of a nucleotide sequence.\n",
    "\n",
    "    Arguments:\n",
    "    sequence -- a string representing the nucleotide sequence\n",
    "\n",
    "    Returns:\n",
    "    The reverse complement of the input sequence, as a string.\n",
    "    \"\"\"\n",
    "    complement = {\"A\": \"T\", \"T\": \"A\", \"C\": \"G\", \"G\": \"C\"}\n",
    "    rev_comp = \"\"\n",
    "    for base in reversed(sequence):\n",
    "        if base not in complement:\n",
    "            raise ValueError(\"Invalid nucleotide: {}\".format(base))\n",
    "        rev_comp += complement[base]\n",
    "    return rev_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c03548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_list_of_dict(filename):\n",
    "    \"\"\"Loading the file that has been used as an input for PRIDICT as a list of dictionaries\"\"\"\n",
    "    result = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            row['id'] = i+1\n",
    "            result.append(row)\n",
    "    return result\n",
    "\n",
    "def append_csv_data(data, folder_path, seq, old = 1, long = 0):\n",
    "    \"\"\"Function to append PRIDICT analysis to each designed pegRNA (we're takig the first row with matching spacer)\n",
    "    seq is the full FASTA sequence of the region that is modified (including the flanking sites)\n",
    "    We perform checks whether spacer from PRIDICT matches the intended SPACER\n",
    "    \n",
    "    We use the flag 'old' to make the code compatible with the old output of PRIDICT\n",
    "    in the old output, the name of Spacer column was 'Spacer-Sequence'\n",
    "    in the new output, the name of Spacer column is 'Protospacer-Sequence'\n",
    "    \n",
    "    We use the flag 'long' to specify the form of the PRIDICT output file\n",
    "    In case we used the long PAM distance, long should be 1\n",
    "    In case we used the short PAM distance, long should be 0\"\"\"\n",
    "    for d in data:\n",
    "        id_value = d['id']\n",
    "        if(long == 0):\n",
    "            filename = folder_path + '/' + str(id_value) + '_pegRNA_Pridict_full.csv'\n",
    "        elif(long == 1):\n",
    "            filename = folder_path + '/' + str(id_value) + '_long_pegRNA_Pridict_full.csv'\n",
    "\n",
    "        if(d['Strand orientation'] == '+' or d['Strand orientation'] == '-'): #If it's not it means we couldn't find a nearby PAM\n",
    "            try:\n",
    "                expected_spacer = find_spacer(seq, d['Strand orientation'], int(d['Position of the first G in the PAM (sense direction)']))\n",
    "                with open(filename, 'r') as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    correct_spacer = 0\n",
    "                    while(correct_spacer == 0): #Search for the first correct spacer (the data is sorted with respect to editing efficiency so we just need to look until the first correct spacer)\n",
    "                        new_data = next(reader) #read the first next row into a dictionary  \n",
    "                        if(old == 1):\n",
    "                            pridict_spacer = new_data['Spacer-Sequence']\n",
    "\n",
    "                        elif(old == 0):\n",
    "                            pridict_spacer = new_data['Protospacer-Sequence']\n",
    "                        if(pridict_spacer == expected_spacer):\n",
    "                            correct_spacer = 1\n",
    "                    d.update(new_data)\n",
    "            except:\n",
    "                #The csv was well output by PRIDICT but there were not cases where expected spacer matched the PRIDICT spacer\n",
    "                print(\"Failed to find a matching spacer for \" + str(id_value))\n",
    "                d.update({'PRIDICT_editing_Score_deep': -1.0}) #we put the score -1 which means this entry should be neglected (otherwise there is an error when searching for highest score)\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to open \" + str(id_value))\n",
    "            d.update({'PRIDICT_editing_Score_deep': -1.0}) #we put the score -1 which means this entry should be neglected (otherwise there is an error when searching for highest score)\n",
    "    return data\n",
    "\n",
    "def group_dict_by_keys(list_of_dicts, keep_only_frequent_codons = 0):\n",
    "    \"\"\"\n",
    "    Takes a list of dictionaries and groups them by the first three keys.\n",
    "    Returns a new list of dictionaries with the grouped dictionaries nested\n",
    "    within their respective parent dictionaries.\n",
    "    \n",
    "    if keep_only_frequent_codons is 1, we keep only the otpimal codon for a mutation\n",
    "    (the rest is not present in the output). For designing a synonymous change,\n",
    "    in case the original codon is already optimal, we take the second most optimal.\n",
    "    In case of W and M, we don't output anything (there are no possible mutations)\n",
    "    \"\"\"\n",
    "    grouped_dict_list = []\n",
    "    i = 0\n",
    "    for d in list_of_dicts:\n",
    "        values = list(d.values())\n",
    "        key = (values[1], values[0], values[3]) #original aa, pos, mutated aa\n",
    "        matching_dict = None\n",
    "        \n",
    "        for group in grouped_dict_list:\n",
    "            group_key = (group['original_aa'], group['pos'], group['mutated_aa'])\n",
    "            if group_key == key:\n",
    "                matching_dict = group\n",
    "                break\n",
    "                \n",
    "        if matching_dict is None:\n",
    "            matching_dict = dict(zip(('original_aa', 'pos', 'mutated_aa'), key))\n",
    "            grouped_dict_list.append(matching_dict)\n",
    "            \n",
    "        if(keep_only_frequent_codons == 0):\n",
    "            matching_dict.setdefault('items', []).append(d)\n",
    "        \n",
    "        if(keep_only_frequent_codons == 1):\n",
    "            if(key[0] != key[2]): #Non-synonynous change\n",
    "                if(d['Mutated_nts'] == most_frequent_codon[d['Mutated_aa']]): #if the codon we're looking at is optimal\n",
    "                    matching_dict.setdefault('items', []).append(d)         \n",
    "                    \n",
    "            elif(key[0] == key[2]): #Synonymous change\n",
    "                if(d['Original_nts'] != most_frequent_codon[d['Original_aa']]): #if existing codon is not optimal\n",
    "                    if(d['Mutated_nts'] == most_frequent_codon[d['Mutated_aa']]): #if we're looking at the optimal codon\n",
    "                        matching_dict.setdefault('items', []).append(d)  \n",
    "                        \n",
    "                if(d['Original_nts'] == most_frequent_codon[d['Original_aa']]): #if existing codon is already optimal\n",
    "                    if(d['Mutated_nts'] == second_most_frequent_codon[d['Mutated_aa']]): #if we're looking at the second best codon\n",
    "                        matching_dict.setdefault('items', []).append(d)\n",
    "        if(i%1000 == 0):\n",
    "            print(i)\n",
    "        i += 1\n",
    "    return grouped_dict_list  \n",
    "\n",
    "def merge_grouped_dicts(list1, list2): #TESTED\n",
    "    \"\"\"Merges two grouped dictionaries\n",
    "    Should be used before finding the best score\n",
    "    We do not perform checks to see if we're adding duplicates - in any case only one will be chosen at the end by max score\"\"\"\n",
    "    merged_list = []\n",
    "    \n",
    "    for dict1 in list1:\n",
    "        merged_dict = dict1.copy()\n",
    "        \n",
    "        for dict2 in list2:\n",
    "            if dict1['original_aa'] == dict2['original_aa'] and dict1['pos'] == dict2['pos'] and dict1['mutated_aa'] == dict2['mutated_aa']:\n",
    "                merged_dict['items'] = dict1.get('items', []) + dict2.get('items', [])\n",
    "        merged_list.append(merged_dict)\n",
    "    \n",
    "    for dict2 in list2:\n",
    "        is_unique = True\n",
    "        \n",
    "        for dict1 in list1:\n",
    "            if dict1['original_aa'] == dict2['original_aa'] and dict1['pos'] == dict2['pos'] and dict1['mutated_aa'] == dict2['mutated_aa']:\n",
    "                is_unique = False\n",
    "                break\n",
    "        \n",
    "        if is_unique:\n",
    "            merged_list.append(dict2)\n",
    "    \n",
    "    return merged_list\n",
    "\n",
    "    \n",
    "def find_highest_score(scores): #TESTED\n",
    "    \"\"\"Finds the dictionary with the highest 'PRIDICT_editing_Score_deep' value' among all dictionaries with the same (original_aa, position, mutated_aa)\"\"\"\n",
    "    highest_score = None\n",
    "    \n",
    "    for score in scores:\n",
    "        if highest_score is None or float(score['PRIDICT_editing_Score_deep']) > float(highest_score['PRIDICT_editing_Score_deep']):\n",
    "            highest_score = score\n",
    "            \n",
    "    return highest_score\n",
    "\n",
    "def print_highest_score_values(data, output_file): #TESTED\n",
    "    # Extract the header keys from the first nested dictionary\n",
    "    items_keys = list(data[0]['items'][0].keys())\n",
    "\n",
    "    # Open the output file and create a CSV writer\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        # Write the header row\n",
    "        writer.writerow(items_keys)\n",
    "\n",
    "        # Loop over the dictionaries in the list\n",
    "        for i,d in enumerate(data):\n",
    "            # Find the dictionary with the highest 'score' value\n",
    "            max_score_dict = find_highest_score(d['items'])\n",
    "\n",
    "            # Write its values to the CSV file\n",
    "            if(max_score_dict != None):\n",
    "                writer.writerow(max_score_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spacer(string, orientation, pam_pos): #TESTED\n",
    "    \"\"\"This function finds spacer based on PAM position\n",
    "    It changes the first nucleotide of the spacer into G, same as PRIDICT\"\"\"\n",
    "    \n",
    "    pam_pos = pam_pos-1 #We load from excel where indexing is 1-based to Python where it's 0-based\n",
    "    if orientation == '+':\n",
    "        start = pam_pos - 21\n",
    "        end = pam_pos - 1\n",
    "        substring = string[start:end]\n",
    "    elif orientation == '-':\n",
    "        start = pam_pos + 3\n",
    "        end = pam_pos + 23\n",
    "        substring = string[start:end]\n",
    "        substring = reverse_complement(substring)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid orientation parameter: must be '+' or '-'\")\n",
    "\n",
    "    return 'G' + substring[1:]  # Change first character to 'G' (this is already performed by PRIDICT output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_whole_mut_seq(seq, original_subseq, mutated_subseq, strand_orientation):\n",
    "    \"\"\"Finds whole mutate sequence (same length as input fasta) \n",
    "    seq: original input fasta\n",
    "    original_subseq: original seq fed to predict\n",
    "    mutated_seq: mutated_seq fed to pridict\n",
    "    strand orientation is needed because in case we use Rv strand, PRIDICT will use reverse complement for original_subseq\n",
    "    and mutated_subseq while seq will be read from Fw strand\"\"\"\n",
    "    \n",
    "    #In newer versions of PRIDICT, the output contains small letters in the edited region\n",
    "    #This is not compatible with the rest of the pipeline so we have convert it to upper\n",
    "    mutated_subseq = mutated_subseq.upper() \n",
    "    \n",
    "    if(strand_orientation == '-'):\n",
    "        original_subseq = reverse_complement(original_subseq)\n",
    "        mutated_subseq = reverse_complement(mutated_subseq)\n",
    "\n",
    "    index = seq.find(original_subseq)\n",
    "    L = len(mutated_subseq)\n",
    "    \n",
    "    seq1 = seq[:index]\n",
    "    seq1 += mutated_subseq\n",
    "    seq1 += seq[index+L:]\n",
    "    \n",
    "    return seq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4691da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutated_seed(original_seq, mutated_seq, pam_pos, orientation, printing = 0): #TESTED\n",
    "    \"\"\"Verifies that the seed is mutated\"\"\"\n",
    "    pam_pos = pam_pos - 1 #We load from excel where indexing is 1-based to Python where it's 0-based\n",
    "    #Inputs should have the same lengths as the input FASTA so that pam_pos is correct\n",
    "    #Note thatt pam_pos is the position of the first G in the case of + orientation\n",
    "    #Or the position of first C in the + strand in the caseof - orientation\n",
    "    if(orientation == \"+\"):\n",
    "        seed_pos = pam_pos - 4\n",
    "    if(orientation == \"-\"):\n",
    "        seed_pos = pam_pos + 3\n",
    "    \n",
    "    seed_original = original_seq[seed_pos:seed_pos + 3]\n",
    "\n",
    "    seed_mutated = mutated_seq[seed_pos:seed_pos+3]\n",
    "    \n",
    "    if(printing == 1):\n",
    "        print('seed original')\n",
    "        print(seed_original)\n",
    "        print('seed mutated')\n",
    "        print(seed_mutated)\n",
    "    if(seed_original != seed_mutated):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e55021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mutated_codon(mutated_seq, aa_pos):\n",
    "    return translate(mutated_seq[aa_pos:aa_pos+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005eba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_pegRNAs(grouped_dict_list, seq):\n",
    "    \"\"\"Removes the dictionaries that are considered bad by fullfilling one of the following criteria:\n",
    "    1. The expected spacer seq does not match the spacer sequence assumed by PRIDICT\n",
    "    2. The seed is not actually mutated\n",
    "    3. (add later) There are unnecessary mutations in the PAM-containing AAs that do not disrupt PAM\"\"\"\n",
    "    new_grouped_dict_list = []\n",
    "    i = 0\n",
    "    for d in grouped_dict_list:\n",
    "        #setting the header of the dict\n",
    "        new_dict = {}\n",
    "        new_dict['original_aa'] = d['original_aa']\n",
    "        new_dict['pos'] = d['pos']\n",
    "        new_dict['mutated_aa'] = d['mutated_aa']\n",
    "        new_dict['items'] = []\n",
    "        \n",
    "        #if(d['original_aa']=='C' and d['pos'] == '270' and d['mutated_aa'] == 'K'): #Was used for testing\n",
    "        #if(d['original_aa']=='S' and d['pos'] == '219' and d['mutated_aa'] == 'S'):\n",
    "        \n",
    "        #iterating over items to filter out bad designs\n",
    "        for item in d['items']:\n",
    "            if(item['PRIDICT_editing_Score_deep'] != -1): #This happens in some cases, we need to avoid them so that it runs through\n",
    "                if(int(item['Total Hamming distance']) > 1):\n",
    "                    expected_mut_aa = item['Mutated_aa']\n",
    "                    whole_mut_seq = reconstruct_whole_mut_seq(seq, item['Original_Sequence'], item['Edited-Sequences'], item['Strand orientation'])\n",
    "                    aa_pos = int(item['Position of the first nt in the aa'])-1 #Converting to Python 0-based indexing\n",
    "                    actual_mut_aa = find_mutated_codon(whole_mut_seq, aa_pos)\n",
    "\n",
    "                    if(expected_mut_aa == actual_mut_aa):\n",
    "                        expected_spacer = find_spacer(seq, item['Strand orientation'], int(item['Position of the first G in the PAM (sense direction)']))\n",
    "                        pridict_spacer = expected_spacer #item['Spacer-Sequence']\n",
    "                        #print('=====')\n",
    "                        #print(item['Position of the first G in the PAM (sense direction)'])\n",
    "                        #print(expected_spacer)\n",
    "                        #print(pridict_spacer)\n",
    "\n",
    "                        #condition 1\n",
    "                        if(expected_spacer == pridict_spacer):\n",
    "                            #condition 2 if seed is expected to be mutated\n",
    "                            if(item['Seed mutated'] == '1' and mutated_seed(seq, whole_mut_seq, int(item['Position of the first G in the PAM (sense direction)']), item['Strand orientation'])):\n",
    "                                #Keep only these\n",
    "                                new_dict['items'].append(item)  \n",
    "                            elif(item['Seed mutated'] == '0'): #if seed is not expected to be mutated, we just print out\n",
    "                                new_dict['items'].append(item)\n",
    "                    else:\n",
    "                        print('Hamming > 1 but AA mut not well designed')\n",
    "                        print(item['id'])\n",
    "                        print(item['Mutated_nts'])\n",
    "                        print(seq[aa_pos-6:aa_pos+9])\n",
    "                        print(seq[aa_pos:aa_pos+3])\n",
    "                        print('Reconstructed mut seq')\n",
    "                        print(whole_mut_seq[aa_pos-6:aa_pos+9])\n",
    "                        print(whole_mut_seq[aa_pos:aa_pos+3])\n",
    "                        print(translate(whole_mut_seq[aa_pos:aa_pos+3]))\n",
    "\n",
    "        new_grouped_dict_list.append(new_dict)\n",
    "        i = i+1\n",
    "    return new_grouped_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a50b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_csv_to_list_of_dict('pegRNA_main_initial_design.csv')\n",
    "seq = read_fasta_file('LDLR ex4_extra seq.fa') #Reading the fasta file (has to be in .fa format and in the same folder as the code)\n",
    "data = append_csv_data(data, 'predictions main', seq, old = 1, long = 0)\n",
    "grouped_dict_list = group_dict_by_keys(data, keep_only_frequent_codons = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_additional_seed = load_csv_to_list_of_dict('pegRNA_design_additional_seed_mutations.csv')\n",
    "data_additional_seed = append_csv_data(data_additional_seed, 'predictions additional seed', seq, old = 0, long = 0)\n",
    "grouped_dict_list2 = group_dict_by_keys(data_additional_seed, keep_only_frequent_codons = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a366bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_two_more_aa = load_csv_to_list_of_dict('pegRNA_design_324_and_327_with_35_limit_mutations.csv')\n",
    "data_two_more_aa  = append_csv_data(data_two_more_aa, 'predictions longer limit', seq, old = 0, long = 1)\n",
    "grouped_dict_list3 = group_dict_by_keys(data_two_more_aa, keep_only_frequent_codons = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_three_more_aa = load_csv_to_list_of_dict('pegRNA_design_264_417_420.csv')\n",
    "data_three_more_aa  = append_csv_data(data_two_more_aa, 'predictions long three more', seq, old = 0, long = 0)\n",
    "grouped_dict_list4 = group_dict_by_keys(data_two_more_aa, keep_only_frequent_codons = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ec400",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dicts_tmp = merge_grouped_dicts(grouped_dict_list, grouped_dict_list2)\n",
    "merged_dicts_tmp2 = merge_grouped_dicts(merged_dicts_tmp, grouped_dict_list3)\n",
    "merged_dicts = merge_grouped_dicts(merged_dicts_tmp2, grouped_dict_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dict_list_new = remove_bad_pegRNAs(merged_dicts, read_fasta_file('LDLR ex4_extra seq.fa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the final output\n",
    "print_highest_score_values(grouped_dict_list_new, 'Filtered_pegRNAs_with_additional_seed_and_longer_limit_for_five_aas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a240c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying potentially missing mutations\n",
    "def find_missing_mutations(csv_file):\n",
    "    \"\"\"Goes through the final output of the pipeline and lists all amino-acids for which a mutation could not be designed\"\"\"\n",
    "    positions = []\n",
    "    mutations = []\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header row\n",
    "\n",
    "        for row in reader:\n",
    "            position = int(row[0])\n",
    "            mutation = row[3]\n",
    "\n",
    "            positions.append(position)\n",
    "            mutations.append(mutation)\n",
    "\n",
    "    unique_positions = set(positions)\n",
    "    missing_mutations = {}\n",
    "\n",
    "    for position in unique_positions:\n",
    "        expected_mutations = set(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '_'])\n",
    "        observed_mutations = set([mutation for idx, mutation in enumerate(mutations) if positions[idx] == position])\n",
    "\n",
    "        missing_mutations[position] = list(expected_mutations - observed_mutations)\n",
    "\n",
    "    return missing_mutations\n",
    "\n",
    "missing_mutations = find_missing_mutations('Filtered_pegRNAs_with_additional_seed_and_longer_limit_for_five_aas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc4e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
